# Модули (слои) и модели для построения нейронных сетей
Слои нейронной сети именуются *модулями*. В torch имеется большое разнообразие встроенных модулей (`nn_module`).

### Линейный слой
Введем обозначения:
* `n_in` – число входов для каждого нейрона слоя (равно числу входных переменных)
* `n_out` – число выходов слоя (равно числу нейронов в слое)
```r
l <- nn_linear(in_features = n_in, out_features = n_out)   ; слой из n_out линейных нейронов, каждый из которых имеет n_in входов
```
Каждый слой `layer` имеет свойсва:
* `layer@wight` – тензор, число строк в котором равно `n_out`, а число столбцов - `n_in`)
* `layer@bias` – тензор из одного столбца с количеством строк, равными `n_out`.
По соображениям производительности линейный модуль хранит тензоры веса и смещения в транспонированной форме.  
В модулях тензоры весов и смещений автоматический создаются с параметром `requires_grad = TRUE`.

Чтобы применить модуль к входным данным, нужно просто вызвать его как функцию от входных данных:
```r
output <- layer(x)
```

### Другие виды модулей
```r
nn_conv1d()     ; одномерный сверточный слой
nn_conv2d()     ; двухмерный сверточный слой
nn_conv3d()     ; трехмерный сверточный слой
nn_lstm()       ; рекуррентный слой
nn_gru()        ; рекуррентный слой
nn_embedding()  ; используется для встраивания категориальных данных в многомерное пространство
```

## Модели нейронных сетей
Модели представляют собой совокупость слоев, представленных модулями `nn_xxx()`.

### Последовательные модели
В случае последовательного распространения сигнала для построения модели из отдельных слоев используют функцию `nn_sequential()`. Все аргументы этой функции должны быть модулями (слоями):
```r
mlp <- nn_sequential(
  nn_linear(10, 32),
  nn_relu(),
  nn_linear(32, 64),
  nn_relu(),
  nn_linear(64, 1)
)
```
Слой `nn_relu()` применяет функцию RELU к каждому входному значению и передает результат на выход. Таким образом число входов и выходов для этого слоя одниаково и не является параметром слоя. Данный слой не имеет параметров. 
Как и для слоя, чтобы применить модель к входным данным достаточно вызвать ее как функцию `mlp(x)` от входных данных. Функция `backward()` выполняет обратное распространение и расчет градиентов.

В примере выше `nn_relu()` представляет собой модуль, в то время как сама функция RELU реализована как `nnf_relu()`.

### Модели с непоследовательным распространением сигнала
Для построения пользовательской модели используется конструктор `nn_module()`.  
При этом для модели необходимо определить две функции:
* `initialize()` – создает внутренние поля модели
* `forward()` – определяет, что должна делать модель при вызове ее на входных данных.
```r
my_linear_model <- nn_module(
  initialize = function(in_features, out_features) {
    self$w <- nn_parameter(torch_randn(in_features, out_features))  ; входной тензор 
    self$b <- nn_parameter(torch_zeros(out_features))
  },
  forward = function(input) { input$mm(self$w) + self$b }
)
```
Для определения внутренних переменных в модели выше использовалась функция `nn_parameter()`, которая гарантирует расчет градиентов для входного тензора.

Чтобы создать экземпляр модели, нужно вызвать его как конструктор: `my_model <- my_linear_model(7, 1)`.
