# Автоматическое дифференцирование
Чтобы тензор, участвующий в расчетах выражения, мог накапливать информацию для расчета автоградиентов, необходимо при его создании указать параметр `requires_grad = TRUE`. Все промежуточные тензоры, учасвующие в расчете выражения будут наследовать этот параметр и не требуют его указания при своем создании. Каждый такой тензор будет иметь свойство `$requires_grad = TRUE`:
```r
t <- torch_tensor(2, requires_grad = TRUE)
```
>*Если при расчете был получен тензор* `result`*, то вызов* `result$backward()` *сделает обратный проход по графу вычислений и произведет расчет градиентов в каждом узле, в том числе и в исходном тензоре. При этом, на тех узлах, где при прямом проходе была вызвана функция* `$retain_grad()`*, результат расчета градиента будет сохраняться – в противном случае для экономии памяти эти результаты будут отброшены после использования.*

Чтобы получить значение градиента для тензора узла достаточно обратиться к его свойству `$grad`, которое вернет значение частной производной результата по значениям данного тензора. Как правило, интересуют только значения градиента по значениям входного тензора, находящелгося в начале графа вычислений. Зная эти градиенты, появляется возможность изменить значения исходного тензора в направлении улучшения результата.

По умолчанию расчеты накапливают полученные градиенты, поэтому на каждом следующем шаге расчета нужно их обнулить. Для этого используется функция `$zero_()`, примененная к свойству `$grad` тензора.
 
#### Пример нахождения минимума функции двух переменных
В качестве исследуемой функции рассмотрим функцию Розенброка, имеющую минимум в узкой расщелине:

$$
f(x, y) = (a - x)^2 + b (y - x^2)^2, a = 1, b = 5
$$

![Функция Розенброка.](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/images/optim-1-rosenbrock.png)
```r
# функция Розенброка; считаем, что x - это вектор с двумя значениями
rosenbrock <- function(x) { return (1 - x[1])^2 + 5 * (x[2] - x[1]^2)^2 }

lr <- 0.01                                          ; приращение параметра на каждой итерации
num_iterations <- 1000                              ; число итераций
x <- torch_tensor(c(-1, 1), requires_grad = TRUE)   ; тензор входных значений функции
for (i in 1:num_iterations) {
  if (i %% 100 == 0) cat("Iteration: ", i, "\n")    ; промещуточный вывод для каждой сотой итерации
  value <- rosenbrock(x)                            ; вычислить значение функции
  if (i %% 100 == 0) {
    cat("Value is: ", as.numeric(value), "\n")      ; для каждой сотой итерации вывести полученное значение функции
  }
  value$backward()                                  ; расчет автоградиента
  if (i %% 100 == 0) {
    cat("Gradient is: ", as.matrix(x$grad), "\n")   ; для каждой сотой итерации вывести полученное значение градиента
  }
  with_no_grad({                                    ; сделать приращение значений тензора входных параметров (без расчета градиентов)
    x$sub_(lr * x$grad)                             ; шаг в направлении обратном градиенту
    x$grad$zero_()                                  ; обнулить значения градиентов тензора x
  })
}
```
